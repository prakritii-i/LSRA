{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb8ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Loading datasets...\n",
      "Found 2434 entries with LYRICS_NOT_FOUND or no lyrics.\n",
      "‚úÖ Failed file updated ‚Üí failed_lyrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# === File paths ===\n",
    "MUSE_CSV = \"muse_final_emotion_dataset.csv\"       # original dataset\n",
    "LYRICS_CSV = \"muse_with_lyrics.csv\"               # after fetching\n",
    "FAILED_CSV = \"failed_lyrics.csv\"                  # existing failed file\n",
    "PROGRESS_JSON = \"progress_smh.json\"                   # tracks traversed\n",
    "SKIPPED_OUT_CSV = \"skipped_refetch.csv\"           # output skipped songs\n",
    "\n",
    "# === Step 1: Move 'lyrics_not_found' from fetched CSV to failed list ===\n",
    "print(\"üî∏ Loading datasets...\")\n",
    "lyrics_df = pd.read_csv(LYRICS_CSV)\n",
    "failed_df = pd.read_csv(FAILED_CSV) if not pd.read_csv(FAILED_CSV).empty else pd.DataFrame(columns=lyrics_df.columns)\n",
    "\n",
    "# Identify rows with lyrics_not_found\n",
    "not_found_mask = lyrics_df['lyrics'].eq('LYRICS_NOT_FOUND') | lyrics_df['lyrics'].isna()\n",
    "not_found_df = lyrics_df[not_found_mask]\n",
    "\n",
    "print(f\"Found {len(not_found_df)} entries with LYRICS_NOT_FOUND or no lyrics.\")\n",
    "\n",
    "# Append them to failed list (avoid duplicates)\n",
    "failed_df = pd.concat([failed_df, not_found_df]).drop_duplicates(subset=[\"track\", \"artist\"])\n",
    "failed_df.to_csv(FAILED_CSV, index=False)\n",
    "print(f\"‚úÖ Failed file updated ‚Üí {FAILED_CSV}\")\n",
    "\n",
    "# Optionally, remove them from lyrics_df if needed (doesn't modify original file here)\n",
    "# lyrics_df = lyrics_df[~not_found_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b83dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['track', 'artist', 'final_emotion', 'lyrics'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4ca42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned dataset saved ‚Üí muse_with_lyrics_cleaned.csv\n",
      "‚ùå Removed 2434 rows (LYRICS_NOT_FOUND or empty lyrics)\n",
      "‚úÖ Remaining rows: 42328 / 44762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "INPUT_CSV = \"muse_with_lyrics.csv\"\n",
    "OUTPUT_CSV = \"muse_with_lyrics_cleaned.csv\"\n",
    "\n",
    "# === Load the dataset ===\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "initial_count = len(df)\n",
    "\n",
    "# === Identify rows to drop ===\n",
    "drop_mask = df['lyrics'].eq('LYRICS_NOT_FOUND') | df['lyrics'].isna()\n",
    "drop_count = drop_mask.sum()\n",
    "\n",
    "# === Filter out unwanted rows ===\n",
    "cleaned_df = df[~drop_mask].copy()\n",
    "final_count = len(cleaned_df)\n",
    "\n",
    "# === Save to new CSV ===\n",
    "cleaned_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"üßπ Cleaned dataset saved ‚Üí {OUTPUT_CSV}\")\n",
    "print(f\"‚ùå Removed {drop_count} rows (LYRICS_NOT_FOUND or empty lyrics)\")\n",
    "print(f\"‚úÖ Remaining rows: {final_count} / {initial_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ba4b4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Emotion distribution after first fetching:\n",
      "final_emotion\n",
      "sadness       6980\n",
      "joy           6916\n",
      "reflective    6127\n",
      "excitement    5965\n",
      "calm          4614\n",
      "neutral       4597\n",
      "romantic      3290\n",
      "anger         2713\n",
      "fear          1126\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "print(\"\\nüìä Emotion distribution after first fetching:\")\n",
    "print(df['final_emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49376409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∏ Extracting skipped songs...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     progress = json.load(f)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# iterate through values (which are the actual song entries)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m traversed_titles = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrack\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m traversed_artists = \u001b[38;5;28mset\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33martist\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m progress.values())\n\u001b[32m     14\u001b[39m traversed_pairs = \u001b[38;5;28mset\u001b[39m((item[\u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m], item[\u001b[33m\"\u001b[39m\u001b[33martist\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m progress.values())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      9\u001b[39m     progress = json.load(f)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# iterate through values (which are the actual song entries)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m traversed_titles = \u001b[38;5;28mset\u001b[39m(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrack\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m progress.values())\n\u001b[32m     13\u001b[39m traversed_artists = \u001b[38;5;28mset\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33martist\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m progress.values())\n\u001b[32m     14\u001b[39m traversed_pairs = \u001b[38;5;28mset\u001b[39m((item[\u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m], item[\u001b[33m\"\u001b[39m\u001b[33martist\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m progress.values())\n",
      "\u001b[31mTypeError\u001b[39m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# === Step 2: Extract skipped songs ===\n",
    "print(\"\\nüî∏ Extracting skipped songs...\")\n",
    "\n",
    "# Load original dataset\n",
    "muse_df = pd.read_csv(MUSE_CSV)\n",
    "\n",
    "# Load progress.json\n",
    "with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    progress = json.load(f)\n",
    "\n",
    "# iterate through values (which are the actual song entries)\n",
    "traversed_titles = set(item[\"track\"] for item in progress.values())\n",
    "traversed_artists = set(item[\"artist\"] for item in progress.values())\n",
    "traversed_pairs = set((item[\"track\"], item[\"artist\"]) for item in progress.values())\n",
    "\n",
    "# Load progress.json\n",
    "# with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "#     progress = json.load(f)\n",
    "\n",
    "# traversed_titles = set(item[\"track\"] for item in progress)\n",
    "# traversed_artists = set(item[\"artist\"] for item in progress)\n",
    "# traversed_pairs = set((item[\"track\"], item[\"artist\"]) for item in progress)\n",
    "\n",
    "# Filter out songs that were traversed\n",
    "mask = muse_df.apply(lambda row: (row[\"track\"], row[\"artist\"]) not in traversed_pairs, axis=1)\n",
    "skipped_df = muse_df[mask]\n",
    "\n",
    "print(f\"Skipped songs found: {len(skipped_df)}\")\n",
    "skipped_df.to_csv(SKIPPED_OUT_CSV, index=False)\n",
    "print(f\"‚úÖ Skipped songs saved ‚Üí {SKIPPED_OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9d1777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[('45', True), ('463', True), ('1923', True)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    progress = json.load(f)\n",
    "\n",
    "print(type(progress))\n",
    "print(list(progress.items())[:3])   # preview first 3 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea3de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original dataset size: 90001\n",
      "‚è∏Ô∏è Skipped entries identified: 0\n",
      "‚è∏Ô∏è Skipped entries identified: 0\n",
      "üíæ Skipped songs saved to: skipped_refetch.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# === File paths ===\n",
    "MUSE_CSV = \"muse_final_emotion_dataset.csv\"\n",
    "PROGRESS_JSON = \"progress_smh.json\"\n",
    "SKIPPED_OUT_CSV = \"skipped_refetch.csv\"\n",
    "\n",
    "# === Load original dataset ===\n",
    "muse_df = pd.read_csv(MUSE_CSV)\n",
    "print(f\"üìÑ Original dataset size: {len(muse_df)}\")\n",
    "\n",
    "# === Load progress.json (all keys are track titles) ===\n",
    "with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    progress = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Optional: handle artist matching as well\n",
    "# Create combined \"track|artist\" keys if you want exact match\n",
    "progress_keys = set(progress.keys())\n",
    "\n",
    "# === Find skipped songs ===\n",
    "# Filter out rows where track (or track|artist) is in progress\n",
    "skipped_df = muse_df[~muse_df['track'].isin(progress_keys)]\n",
    "print(f\"‚è∏Ô∏è Skipped entries identified: {len(skipped_df)}\")\n",
    "\n",
    "# === Save skipped songs ===\n",
    "skipped_df.to_csv(SKIPPED_OUT_CSV, index=False)\n",
    "print(f\"üíæ Skipped songs saved to: {SKIPPED_OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffee7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys from progress.json: [\"'Till I Collapse\", 'St. Anger', \"Speedin'\", 'Bamboo Banga', 'Die MF Die', 'Step Up', 'Feedback', '7 Words', 'Limp', 'Sweet Amber']\n",
      "Sample tracks from dataset: [\"'Till I Collapse\", 'St. Anger', \"Speedin'\", 'Bamboo Banga', 'Die MF Die', 'Step Up', 'Feedback', '7 Words', 'Limp', 'Sweet Amber']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample keys from progress.json:\", list(progress.keys())[:10])\n",
    "print(\"Sample tracks from dataset:\", muse_df['track'].head(10).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca3cda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original dataset size: 90001\n",
      "‚è∏Ô∏è Skipped entries identified: 121\n",
      "üíæ Skipped songs saved to: skipped_refetch.csv\n",
      "üíæ Skipped songs saved to: c:\\Users\\LENOVO\\Desktop\\LSRA\\Sentiment_Classification\\Lyrics_Fetching\\updated\\skipped_refetch.csv\n",
      "\n",
      "üìÑ CSV File Content Preview:\n",
      "\n",
      "artist,final_emotion,track\n",
      "Dry Kill Logic,anger,4039\n",
      "Elvis Costello,anger,45\n",
      "Buck 65,anger,463\n",
      "Marissa Nadler,sadness,1923\n",
      "Decapitated,excitement,404\n",
      "–°–ø–ª–∏–Ω,excitement,3007\n",
      "Daturah,excitement,9\n",
      "Sunny Day Real Estate,excitement,9\n",
      "Cornelius,excitement,2010\n",
      "Squarepusher,reflective,4001\n",
      "ÂùÇÊú¨Èæç‰∏Ä,anger,1919\n",
      "Caf√© Tacvba,sadness,53100\n",
      "Bohren & der Club of Gore,sadness,3\n",
      "The Dresden Dolls,reflective,672\n",
      "Interpol,fear,5\n",
      "Emilie Autumn,fear,306\n",
      "Sedativ,sadness,9\n",
      "The Leather Nun,sadness,506\n",
      "Mattafix,fear,1130\n",
      "Th\n",
      "Preview of skipped entries:\n",
      "              artist final_emotion track\n",
      "503   Dry Kill Logic         anger  4039\n",
      "1740  Elvis Costello         anger    45\n",
      "1772         Buck 65         anger   463\n",
      "2670  Marissa Nadler       sadness  1923\n",
      "5860     Decapitated    excitement   404\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "# === File paths ===\n",
    "MUSE_CSV = \"muse_final_emotion_dataset.csv\"\n",
    "PROGRESS_JSON = \"progress_smh.json\"\n",
    "SKIPPED_OUT_CSV = \"skipped_refetch.csv\"\n",
    "\n",
    "# === Helper: Normalize strings ===\n",
    "def normalize_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    return unicodedata.normalize('NFKC', s.strip().lower())\n",
    "\n",
    "# === Load original dataset ===\n",
    "muse_df = pd.read_csv(MUSE_CSV)\n",
    "print(f\"üìÑ Original dataset size: {len(muse_df)}\")\n",
    "\n",
    "# Normalize track titles in dataset\n",
    "muse_df[\"track_normalized\"] = muse_df[\"track\"].apply(normalize_text)\n",
    "\n",
    "# === Load progress.json ===\n",
    "with open(PROGRESS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    progress = json.load(f)\n",
    "\n",
    "# Normalize progress keys\n",
    "progress_keys_normalized = set(normalize_text(k) for k in progress.keys())\n",
    "\n",
    "# === Filter skipped songs ===\n",
    "# Create a mask instead of slicing first\n",
    "skipped_mask = ~muse_df[\"track_normalized\"].isin(progress_keys_normalized)\n",
    "\n",
    "# Then apply to the original dataset (without the normalized column)\n",
    "skipped_df = muse_df.loc[skipped_mask, muse_df.columns.difference([\"track_normalized\"])]\n",
    "print(f\"‚è∏Ô∏è Skipped entries identified: {len(skipped_df)}\")\n",
    "\n",
    "# === Save to CSV ===\n",
    "skipped_df.to_csv(SKIPPED_OUT_CSV, index=False)\n",
    "print(f\"üíæ Skipped songs saved to: {SKIPPED_OUT_CSV}\")\n",
    "\n",
    "import os\n",
    "\n",
    "skipped_df.to_csv(SKIPPED_OUT_CSV, index=False)\n",
    "print(f\"üíæ Skipped songs saved to: {os.path.abspath(SKIPPED_OUT_CSV)}\")\n",
    "\n",
    "# Double-check file content\n",
    "with open(SKIPPED_OUT_CSV, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    print(\"\\nüìÑ CSV File Content Preview:\\n\")\n",
    "    print(content[:500] if content else \"[Empty file]\")\n",
    "\n",
    "\n",
    "print(\"Preview of skipped entries:\")\n",
    "print(skipped_df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc8987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 9614 rows after pre-cleaning\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "INPUT_CSV = \"akipped_muse_with_lyrics.csv\"\n",
    "TEMP_CSV = \"skipped_temp_clean.csv\"\n",
    "\n",
    "with open(INPUT_CSV, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
    "     open(TEMP_CSV, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        if len(row) >= 3:    # make sure track, artist, lyrics exist\n",
    "            writer.writerow(row[:3])  # truncate any extra messy fields\n",
    "\n",
    "# Now load the cleaned CSV\n",
    "df = pd.read_csv(TEMP_CSV)\n",
    "print(f\"‚úÖ Loaded {len(df)} rows after pre-cleaning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c29e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned dataset saved ‚Üí skipped_muse_with_lyrics_cleaned.csv\n",
      "‚ùå Removed 2496 rows (LYRICS_NOT_FOUND or empty lyrics)\n",
      "‚úÖ Remaining rows: 7118 / 9614\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "INPUT_CSV = \"skipped_temp_clean.csv\"\n",
    "OUTPUT_CSV = \"skipped_muse_with_lyrics_cleaned.csv\"\n",
    "\n",
    "# === Load the dataset safely ===\n",
    "df = pd.read_csv(INPUT_CSV, quotechar='\"', escapechar='\\\\')\n",
    "initial_count = len(df)\n",
    "\n",
    "# === Clean lyrics column and identify rows to drop ===\n",
    "df['lyrics'] = df['lyrics'].astype(str).str.strip()\n",
    "drop_mask = (df['lyrics'].eq('LYRICS_NOT_FOUND')) | (df['lyrics'].isna()) | (df['lyrics'].eq('nan'))\n",
    "drop_count = drop_mask.sum()\n",
    "\n",
    "# === Filter out unwanted rows ===\n",
    "cleaned_df = df[~drop_mask].copy()\n",
    "final_count = len(cleaned_df)\n",
    "\n",
    "# === Save to new CSV ===\n",
    "cleaned_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"üßπ Cleaned dataset saved ‚Üí {OUTPUT_CSV}\")\n",
    "print(f\"‚ùå Removed {drop_count} rows (LYRICS_NOT_FOUND or empty lyrics)\")\n",
    "print(f\"‚úÖ Remaining rows: {final_count} / {initial_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
